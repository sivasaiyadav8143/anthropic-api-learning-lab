{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for prompt evaluation framework\n",
    "import json                         # JSON parsing and serialization\n",
    "import concurrent.futures          # Parallel execution of tasks\n",
    "import re                           # Regular expressions for template variable replacement\n",
    "from textwrap import dedent         # Clean up indentation in multiline strings\n",
    "from statistics import mean         # Calculate average scores\n",
    "from dotenv import load_dotenv      # Load environment variables from .env file\n",
    "from anthropic import Anthropic     # Anthropic API client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Anthropic client and define helper functions for message management\n",
    "\n",
    "# Load environment variables (e.g., API key for Anthropic) from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create an instance of the Anthropic client; it will read the API key from the environment\n",
    "client = Anthropic()\n",
    "\n",
    "# Specify the model to use for all chat calls\n",
    "model = \"claude-haiku-4-5\"\n",
    "\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    \"\"\"Add a user message to the conversation history\"\"\"\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    \"\"\"Add an assistant message to the conversation history\"\"\"\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    \"\"\"\n",
    "    Send a chat request to the Anthropic API and return the text response.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content' keys\n",
    "        system: Optional system prompt to guide the model behavior\n",
    "        temperature: Sampling temperature (0.0 = deterministic, 1.0+ = more random)\n",
    "        stop_sequences: List of strings where the model should stop generation\n",
    "    \n",
    "    Returns:\n",
    "        The generated text response from the model\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    # Include system prompt only if provided\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    # Make the API call and extract the text from the response\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML Report Generator for Prompt Evaluation Results\n",
    "# This function converts evaluation results into a formatted HTML report with summary statistics and detailed results\n",
    "\n",
    "def generate_prompt_evaluation_report(evaluation_results):\n",
    "    \"\"\"\n",
    "    Generate an HTML report from evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: List of result dicts, each containing score, output, test_case, and reasoning\n",
    "    \n",
    "    Returns:\n",
    "        HTML string ready to be written to a file\n",
    "    \"\"\"\n",
    "    # Extract scores and compute summary statistics\n",
    "    total_tests = len(evaluation_results)\n",
    "    scores = [result[\"score\"] for result in evaluation_results]\n",
    "    avg_score = mean(scores) if scores else 0\n",
    "    max_possible_score = 10\n",
    "    pass_rate = (\n",
    "        100 * len([s for s in scores if s >= 7]) / total_tests if total_tests else 0\n",
    "    )\n",
    "\n",
    "    # Start building HTML document with embedded CSS styling\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>Prompt Evaluation Report</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                line-height: 1.6;\n",
    "                margin: 0;\n",
    "                padding: 20px;\n",
    "                color: #333;\n",
    "            }}\n",
    "            .header {{\n",
    "                background-color: #f0f0f0;\n",
    "                padding: 20px;\n",
    "                border-radius: 5px;\n",
    "                margin-bottom: 20px;\n",
    "            }}\n",
    "            .summary-stats {{\n",
    "                display: flex;\n",
    "                justify-content: space-between;\n",
    "                flex-wrap: wrap;\n",
    "                gap: 10px;\n",
    "            }}\n",
    "            .stat-box {{\n",
    "                background-color: #fff;\n",
    "                border-radius: 5px;\n",
    "                padding: 15px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "                flex-basis: 30%;\n",
    "                min-width: 200px;\n",
    "            }}\n",
    "            .stat-value {{\n",
    "                font-size: 24px;\n",
    "                font-weight: bold;\n",
    "                margin-top: 5px;\n",
    "            }}\n",
    "            table {{\n",
    "                width: 100%;\n",
    "                border-collapse: collapse;\n",
    "                margin-top: 20px;\n",
    "            }}\n",
    "            th {{\n",
    "                background-color: #4a4a4a;\n",
    "                color: white;\n",
    "                text-align: left;\n",
    "                padding: 12px;\n",
    "            }}\n",
    "            td {{\n",
    "                padding: 10px;\n",
    "                border-bottom: 1px solid #ddd;\n",
    "                vertical-align: top;\n",
    "            }}\n",
    "            tr:nth-child(even) {{\n",
    "                background-color: #f9f9f9;\n",
    "            }}\n",
    "            .output-cell {{\n",
    "                white-space: pre-wrap;\n",
    "            }}\n",
    "            .score {{\n",
    "                font-weight: bold;\n",
    "                padding: 5px 10px;\n",
    "                border-radius: 3px;\n",
    "                display: inline-block;\n",
    "            }}\n",
    "            .score-high {{\n",
    "                background-color: #c8e6c9;\n",
    "                color: #2e7d32;\n",
    "            }}\n",
    "            .score-medium {{\n",
    "                background-color: #fff9c4;\n",
    "                color: #f57f17;\n",
    "            }}\n",
    "            .score-low {{\n",
    "                background-color: #ffcdd2;\n",
    "                color: #c62828;\n",
    "            }}\n",
    "            .output {{\n",
    "                overflow: auto;\n",
    "                white-space: pre-wrap;\n",
    "            }}\n",
    "\n",
    "            .output pre {{\n",
    "                background-color: #f5f5f5;\n",
    "                border: 1px solid #ddd;\n",
    "                border-radius: 4px;\n",
    "                padding: 10px;\n",
    "                margin: 0;\n",
    "                font-family: 'Consolas', 'Monaco', 'Courier New', monospace;\n",
    "                font-size: 14px;\n",
    "                line-height: 1.4;\n",
    "                color: #333;\n",
    "                box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);\n",
    "                overflow-x: auto;\n",
    "                white-space: pre-wrap; \n",
    "                word-wrap: break-word; \n",
    "            }}\n",
    "\n",
    "            td {{\n",
    "                width: 20%;\n",
    "            }}\n",
    "            .score-col {{\n",
    "                width: 80px;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"header\">\n",
    "            <h1>Prompt Evaluation Report</h1>\n",
    "            <div class=\"summary-stats\">\n",
    "                <!-- Display key summary statistics -->\n",
    "                <div class=\"stat-box\">\n",
    "                    <div>Total Test Cases</div>\n",
    "                    <div class=\"stat-value\">{total_tests}</div>\n",
    "                </div>\n",
    "                <div class=\"stat-box\">\n",
    "                    <div>Average Score</div>\n",
    "                    <div class=\"stat-value\">{avg_score:.1f} / {max_possible_score}</div>\n",
    "                </div>\n",
    "                <div class=\"stat-box\">\n",
    "                    <div>Pass Rate (≥7)</div>\n",
    "                    <div class=\"stat-value\">{pass_rate:.1f}%</div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Scenario</th>\n",
    "                    <th>Prompt Inputs</th>\n",
    "                    <th>Solution Criteria</th>\n",
    "                    <th>Output</th>\n",
    "                    <th>Score</th>\n",
    "                    <th>Reasoning</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through each result and build table rows\n",
    "    for result in evaluation_results:\n",
    "        # Format prompt inputs as HTML list\n",
    "        prompt_inputs_html = \"<br>\".join(\n",
    "            [\n",
    "                f\"<strong>{key}:</strong> {value}\"\n",
    "                for key, value in result[\"test_case\"][\"prompt_inputs\"].items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Format solution criteria as bulleted list\n",
    "        criteria_string = \"<br>• \".join(result[\"test_case\"][\"solution_criteria\"])\n",
    "\n",
    "        # Apply color coding to score based on performance threshold\n",
    "        score = result[\"score\"]\n",
    "        if score >= 8:\n",
    "            score_class = \"score-high\"\n",
    "        elif score <= 5:\n",
    "            score_class = \"score-low\"\n",
    "        else:\n",
    "            score_class = \"score-medium\"\n",
    "\n",
    "        # Build table row HTML for this result\n",
    "        html += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{result[\"test_case\"][\"scenario\"]}</td>\n",
    "                <td class=\"prompt-inputs\">{prompt_inputs_html}</td>\n",
    "                <td class=\"criteria\">• {criteria_string}</td>\n",
    "                <td class=\"output\"><pre>{result[\"output\"]}</pre></td>\n",
    "                <td class=\"score-col\"><span class=\"score {score_class}\">{score}</span></td>\n",
    "                <td class=\"reasoning\">{result[\"reasoning\"]}</td>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "\n",
    "    # Close the HTML document\n",
    "    html += \"\"\"\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptEvaluator Implementation\n",
    "# Comprehensive class for generating test datasets and evaluating prompt performance\n",
    "\n",
    "class PromptEvaluator:\n",
    "    \"\"\"\n",
    "    A framework for generating test cases and evaluating prompt quality.\n",
    "    Handles dataset generation, test case execution, grading, and report generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_concurrent_tasks=3):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with concurrency settings.\n",
    "        \n",
    "        Args:\n",
    "            max_concurrent_tasks: Number of parallel tasks to execute (default: 3)\n",
    "        \"\"\"\n",
    "        self.max_concurrent_tasks = max_concurrent_tasks\n",
    "\n",
    "    def render(self, template_string, variables):\n",
    "        \"\"\"\n",
    "        Simple template variable substitution using {variable} syntax.\n",
    "        \n",
    "        Args:\n",
    "            template_string: String with {variable} placeholders\n",
    "            variables: Dictionary of variable names and values\n",
    "        \n",
    "        Returns:\n",
    "            String with variables replaced\n",
    "        \"\"\"\n",
    "        # Find all placeholder variables in the template\n",
    "        placeholders = re.findall(r\"{([^{}]+)}\", template_string)\n",
    "\n",
    "        result = template_string\n",
    "        # Replace each placeholder with its corresponding value\n",
    "        for placeholder in placeholders:\n",
    "            if placeholder in variables:\n",
    "                result = result.replace(\n",
    "                    \"{\" + placeholder + \"}\", str(variables[placeholder])\n",
    "                )\n",
    "\n",
    "        # Clean up any escaped braces ({{ and }})\n",
    "        return result.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "\n",
    "    def generate_unique_ideas(self, task_description, prompt_inputs_spec, num_cases):\n",
    "        \"\"\"Generate a list of unique ideas for testing a prompt based on the task description\"\"\"\n",
    "\n",
    "        # Prompt instructs the model to generate diverse test scenario ideas\n",
    "        prompt = \"\"\"\n",
    "        Generate {num_cases} unique, diverse ideas for testing a prompt that accomplishes this task:\n",
    "        \n",
    "        <task_description>\n",
    "        {task_description}\n",
    "        </task_description>\n",
    "\n",
    "        The prompt will receive the following inputs\n",
    "        <prompt_inputs>\n",
    "        {prompt_inputs_spec}\n",
    "        </prompt_inputs>\n",
    "        \n",
    "        Each idea should represent a distinct scenario or example that tests different aspects of the task.\n",
    "        \n",
    "        Output Format:\n",
    "        Provide your response as a structured JSON array where each item is a brief description of the idea.\n",
    "        \n",
    "        Example:\n",
    "        ```json\n",
    "        [\n",
    "            \"Testing with technical computer science terminology\",\n",
    "            \"Testing with medical research findings\",\n",
    "            \"Testing with complex mathematical concepts\",\n",
    "            ...\n",
    "        ]\n",
    "        ```\n",
    "        \n",
    "        Ensure each idea is:\n",
    "        - Clearly distinct from the others\n",
    "        - Relevant to the task description\n",
    "        - Specific enough to guide generation of a full test case\n",
    "        - Quick to solve without requiring extensive computation or multi-step processing\n",
    "        - Solvable with no more than 400 tokens of output\n",
    "\n",
    "        Remember, only generate {num_cases} unique ideas\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = \"You are a test scenario designer specialized in creating diverse, unique testing scenarios.\"\n",
    "\n",
    "        # Format the prompt inputs specification for display in the prompt\n",
    "        example_prompt_inputs = \"\"\n",
    "        for key, value in prompt_inputs_spec.items():\n",
    "            val = value.replace(\"\\n\", \"\\\\n\")\n",
    "            example_prompt_inputs += f'\"{key}\": str # {val},'\n",
    "\n",
    "        # Render the template with actual task description and configuration\n",
    "        rendered_prompt = self.render(\n",
    "            dedent(prompt),\n",
    "            {\n",
    "                \"task_description\": task_description,\n",
    "                \"num_cases\": num_cases,\n",
    "                \"prompt_inputs\": example_prompt_inputs,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Send to model and parse JSON response\n",
    "        messages = []\n",
    "        add_user_message(messages, rendered_prompt)\n",
    "        add_assistant_message(messages, \"```json\")\n",
    "        text = chat(\n",
    "            messages,\n",
    "            stop_sequences=[\"```\"],\n",
    "            system=system_prompt,\n",
    "            temperature=1.0,\n",
    "        )\n",
    "\n",
    "        return json.loads(text)\n",
    "\n",
    "    def generate_test_case(self, task_description, idea, prompt_inputs_spec={}):\n",
    "        \"\"\"Generate a single test case based on the task description and a specific idea\"\"\"\n",
    "\n",
    "        # Build example input format for the prompt\n",
    "        example_prompt_inputs = \"\"\n",
    "        for key, value in prompt_inputs_spec.items():\n",
    "            val = value.replace(\"\\n\", \"\\\\n\")\n",
    "            example_prompt_inputs += f'\"{key}\": \"EXAMPLE_VALUE\", // {val}\\n'\n",
    "\n",
    "        # Create a comma-separated list of allowed input keys\n",
    "        allowed_keys = \", \".join([f'\"{key}\"' for key in prompt_inputs_spec.keys()])\n",
    "\n",
    "        # Detailed prompt for generating a complete test case\n",
    "        prompt = \"\"\"\n",
    "        Generate a single detailed test case for a prompt evaluation based on:\n",
    "        \n",
    "        <task_description>\n",
    "        {task_description}\n",
    "        </task_description>\n",
    "        \n",
    "        <specific_idea>\n",
    "        {idea}\n",
    "        </specific_idea>\n",
    "        \n",
    "        <allowed_input_keys>\n",
    "        {allowed_keys}\n",
    "        </allowed_input_keys>\n",
    "        \n",
    "        Output Format:\n",
    "        ```json\n",
    "        {{\n",
    "            \"prompt_inputs\": {{\n",
    "            {example_prompt_inputs}\n",
    "            }},\n",
    "            \"solution_criteria\": [\"criterion 1\", \"criterion 2\", ...] // Concise list of criteria for evaluating the solution, 1 to 4 items\n",
    "        }}\n",
    "        ```\n",
    "        \n",
    "        IMPORTANT REQUIREMENTS:\n",
    "        - You MUST ONLY use these exact input keys in your prompt_inputs: {allowed_keys}        \n",
    "        - Do NOT add any additional keys to prompt_inputs\n",
    "        - All keys listed in allowed_input_keys must be included in your response\n",
    "        - Make the test case realistic and practically useful\n",
    "        - Include measurable, concise solution criteria\n",
    "        - The solution criteria should ONLY address the direct requirements of the task description and the generated prompt_inputs\n",
    "        - Avoid over-specifying criteria with requirements that go beyond the core task\n",
    "        - Keep solution criteria simple, focused, and directly tied to the fundamental task\n",
    "        - The test case should be tailored to the specific idea provided\n",
    "        - Quick to solve without requiring extensive computation or multi-step processing\n",
    "        - Solvable with no more than 400 tokens of output\n",
    "        - DO NOT include any fields beyond those specified in the output format\n",
    "\n",
    "        Here's an example of a sample input with an ideal output:\n",
    "        <sample_input>\n",
    "        <sample_task_description>\n",
    "        Extract topics out of a passage of text\n",
    "        </sample_task_description>\n",
    "        <sample_specific_idea>\n",
    "        Testing with a text that contains multiple nested topics and subtopics (e.g., a passage about renewable energy that covers solar power economics, wind turbine technology, and policy implications simultaneously)\n",
    "        </sample_specific_idea>\n",
    "\n",
    "        <sample_allowed_input_keys>\n",
    "        \"content\"\n",
    "        </sample_allowed_input_keys>\n",
    "        </sample_input>\n",
    "        <ideal_output>\n",
    "        ```json\n",
    "        {\n",
    "            \"prompt_inputs\": {\n",
    "                \"content\": \"The transition to renewable energy encompasses numerous interdependent dimensions. Solar photovoltaic technology has seen dramatic cost reductions, with panel efficiency improving 24% since 2010 while manufacturing costs declined by 89%, making it economically competitive with fossil fuels in many markets. Concurrently, wind energy has evolved through innovative turbine designs featuring carbon-fiber composite blades and advanced control systems that increase energy capture by 35% in low-wind conditions.\"\n",
    "            },\n",
    "            \"solution_criteria\": [\n",
    "                \"Includes all topics mentioned\"   \n",
    "            ]\n",
    "        }\n",
    "        ```\n",
    "        </ideal_output>\n",
    "        This is ideal output because the solution criteria is concise and doesn't ask for anything outside of the scope of the task description.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = \"You are a test case creator specializing in designing evaluation scenarios.\"\n",
    "\n",
    "        # Render template with task, idea, and input keys\n",
    "        rendered_prompt = self.render(\n",
    "            dedent(prompt),\n",
    "            {\n",
    "                \"allowed_keys\": allowed_keys,\n",
    "                \"task_description\": task_description,\n",
    "                \"idea\": idea,\n",
    "                \"example_prompt_inputs\": example_prompt_inputs,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Send to model and parse JSON response\n",
    "        messages = []\n",
    "        add_user_message(messages, rendered_prompt)\n",
    "        add_assistant_message(messages, \"```json\")\n",
    "        text = chat(\n",
    "            messages,\n",
    "            stop_sequences=[\"```\"],\n",
    "            system=system_prompt,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        # Parse and augment the test case with metadata\n",
    "        test_case = json.loads(text)\n",
    "        test_case[\"task_description\"] = task_description\n",
    "        test_case[\"scenario\"] = idea\n",
    "\n",
    "        return test_case\n",
    "\n",
    "    def generate_dataset(\n",
    "        self,\n",
    "        task_description,\n",
    "        prompt_inputs_spec={},\n",
    "        num_cases=1,\n",
    "        output_file=\"dataset.json\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate a complete test dataset and save it to a file.\n",
    "        \n",
    "        Args:\n",
    "            task_description: Description of the prompt's task\n",
    "            prompt_inputs_spec: Dictionary defining required input parameters\n",
    "            num_cases: Number of test cases to generate\n",
    "            output_file: Path to save the JSON dataset\n",
    "        \n",
    "        Returns:\n",
    "            List of generated test cases\n",
    "        \"\"\"\n",
    "        # Step 1: Generate diverse testing ideas based on task description\n",
    "        ideas = self.generate_unique_ideas(\n",
    "            task_description, prompt_inputs_spec, num_cases\n",
    "        )\n",
    "\n",
    "        dataset = []\n",
    "        completed = 0\n",
    "        total = len(ideas)\n",
    "        last_reported_percentage = 0\n",
    "\n",
    "        # Step 2: Parallel generation of test cases from each idea\n",
    "        with concurrent.futures.ThreadPoolExecutor(\n",
    "            max_workers=self.max_concurrent_tasks\n",
    "        ) as executor:\n",
    "            # Submit all test case generation tasks\n",
    "            future_to_idea = {\n",
    "                executor.submit(\n",
    "                    self.generate_test_case,\n",
    "                    task_description,\n",
    "                    idea,\n",
    "                    prompt_inputs_spec,\n",
    "                ): idea\n",
    "                for idea in ideas\n",
    "            }\n",
    "\n",
    "            # Collect results as they complete\n",
    "            for future in concurrent.futures.as_completed(future_to_idea):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    completed += 1\n",
    "                    current_percentage = int((completed / total) * 100)\n",
    "                    milestone_percentage = (current_percentage // 20) * 20\n",
    "\n",
    "                    # Print progress at 20% milestones\n",
    "                    if milestone_percentage > last_reported_percentage:\n",
    "                        print(f\"Generated {completed}/{total} test cases\")\n",
    "                        last_reported_percentage = milestone_percentage\n",
    "\n",
    "                    dataset.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating test case: {e}\")\n",
    "\n",
    "        # Step 3: Save dataset to JSON file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def grade_output(self, test_case, output, extra_criteria):\n",
    "        \"\"\"\n",
    "        Grade a model's output using the Anthropic API as an evaluator.\n",
    "        \n",
    "        Args:\n",
    "            test_case: Dictionary containing task_description, prompt_inputs, and solution_criteria\n",
    "            output: The model's generated output to evaluate\n",
    "            extra_criteria: Optional string with additional mandatory requirements\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with strengths, weaknesses, reasoning, and score (1-10)\n",
    "        \"\"\"\n",
    "\n",
    "        # Format prompt inputs for display in the evaluation prompt\n",
    "        prompt_inputs = \"\"\n",
    "        for key, value in test_case[\"prompt_inputs\"].items():\n",
    "            val = value.replace(\"\\n\", \"\\\\n\")\n",
    "            prompt_inputs += f'\"{key}\":\"{val}\",\\n'\n",
    "\n",
    "        # Build extra criteria section if provided\n",
    "        extra_criteria_section = \"\"\n",
    "        if extra_criteria:\n",
    "            extra_criteria_template = \"\"\"\n",
    "            Mandatory Requirements - ANY VIOLATION MEANS AUTOMATIC FAILURE (score of 3 or lower):\n",
    "            <extra_important_criteria>\n",
    "            {extra_criteria}\n",
    "            </extra_important_criteria>\n",
    "            \"\"\"\n",
    "            extra_criteria_section = self.render(\n",
    "                dedent(extra_criteria_template),\n",
    "                {\"extra_criteria\": extra_criteria},\n",
    "            )\n",
    "\n",
    "        # Build the evaluation prompt template\n",
    "        eval_template = \"\"\"\n",
    "        Your task is to evaluate the following AI-generated solution with EXTREME RIGOR.\n",
    "\n",
    "        Original task description:\n",
    "        <task_description>\n",
    "        {task_description}\n",
    "        </task_description>\n",
    "\n",
    "        Original task inputs:\n",
    "        <task_inputs>\n",
    "        {{ {prompt_inputs} }}\n",
    "        </task_inputs>\n",
    "\n",
    "        Solution to Evaluate:\n",
    "        <solution>\n",
    "        {output}\n",
    "        </solution>\n",
    "\n",
    "        Criteria you should use to evaluate the solution:\n",
    "        <criteria>\n",
    "        {solution_criteria}\n",
    "        </criteria>\n",
    "\n",
    "        {extra_criteria_section}\n",
    "\n",
    "        Scoring Guidelines:\n",
    "        * Score 1-3: Solution fails to meet one or more MANDATORY requirements\n",
    "        * Score 4-6: Solution meets all mandatory requirements but has significant deficiencies in secondary criteria\n",
    "        * Score 7-8: Solution meets all mandatory requirements and most secondary criteria, with minor issues\n",
    "        * Score 9-10: Solution meets all mandatory and secondary criteria\n",
    "\n",
    "        IMPORTANT SCORING INSTRUCTIONS:\n",
    "        * Grade the output based ONLY on the listed criteria. Do not add your own extra requirements.\n",
    "        * If a solution meets all of the mandatory and secondary criteria give it a 10\n",
    "        * Don't complain that the solution \"only\" meets the mandatory and secondary criteria. Solutions shouldn't go above and beyond - they should meet the exact listed criteria.\n",
    "        * ANY violation of a mandatory requirement MUST result in a score of 3 or lower\n",
    "        * The full 1-10 scale should be utilized - don't hesitate to give low scores when warranted\n",
    "\n",
    "        Output Format\n",
    "        Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "        - \"strengths\": An array of 1-3 key strengths\n",
    "        - \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "        - \"reasoning\": A concise explanation of your overall assessment\n",
    "        - \"score\": A number between 1-10\n",
    "\n",
    "        Respond with JSON. Keep your response concise and direct.\n",
    "        Example response shape:\n",
    "        {{\n",
    "            \"strengths\": string[],\n",
    "            \"weaknesses\": string[],\n",
    "            \"reasoning\": string,\n",
    "            \"score\": number\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        # Render the evaluation template with actual data\n",
    "        eval_prompt = self.render(\n",
    "            dedent(eval_template),\n",
    "            {\n",
    "                \"task_description\": test_case[\"task_description\"],\n",
    "                \"prompt_inputs\": prompt_inputs,\n",
    "                \"output\": output,\n",
    "                \"solution_criteria\": \"\\n\".join(test_case[\"solution_criteria\"]),\n",
    "                \"extra_criteria_section\": extra_criteria_section,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Send evaluation prompt to model and parse JSON response\n",
    "        messages = []\n",
    "        add_user_message(messages, eval_prompt)\n",
    "        add_assistant_message(messages, \"```json\")\n",
    "        eval_text = chat(\n",
    "            messages,\n",
    "            stop_sequences=[\"```\"],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        return json.loads(eval_text)\n",
    "\n",
    "    def run_test_case(self, test_case, run_prompt_function, extra_criteria=None):\n",
    "        \"\"\"\n",
    "        Execute a single test case: run the prompt and grade the result.\n",
    "        \n",
    "        Args:\n",
    "            test_case: Test case dictionary with task_description and prompt_inputs\n",
    "            run_prompt_function: Callable that takes prompt_inputs and returns output\n",
    "            extra_criteria: Optional extra evaluation criteria\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with output, test_case, score, and reasoning\n",
    "        \"\"\"\n",
    "        # Step 1: Execute the prompt with the test case inputs\n",
    "        output = run_prompt_function(test_case[\"prompt_inputs\"])\n",
    "\n",
    "        # Step 2: Grade the output using the model as evaluator\n",
    "        model_grade = self.grade_output(test_case, output, extra_criteria)\n",
    "        model_score = model_grade[\"score\"]\n",
    "        reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "        # Return structured result\n",
    "        return {\n",
    "            \"output\": output,\n",
    "            \"test_case\": test_case,\n",
    "            \"score\": model_score,\n",
    "            \"reasoning\": reasoning,\n",
    "        }\n",
    "\n",
    "    def run_evaluation(\n",
    "        self,\n",
    "        run_prompt_function,\n",
    "        dataset_file,\n",
    "        extra_criteria=None,\n",
    "        json_output_file=\"output.json\",\n",
    "        html_output_file=\"output.html\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the complete evaluation suite on all test cases in the dataset.\n",
    "        \n",
    "        Args:\n",
    "            run_prompt_function: Function to test (takes prompt_inputs, returns output)\n",
    "            dataset_file: Path to the JSON dataset file\n",
    "            extra_criteria: Optional extra evaluation criteria string\n",
    "            json_output_file: Where to save JSON results\n",
    "            html_output_file: Where to save HTML report\n",
    "        \n",
    "        Returns:\n",
    "            List of result dictionaries\n",
    "        \"\"\"\n",
    "        # Load the dataset from file\n",
    "        with open(dataset_file, \"r\") as f:\n",
    "            dataset = json.load(f)\n",
    "\n",
    "        results = []\n",
    "        completed = 0\n",
    "        total = len(dataset)\n",
    "        last_reported_percentage = 0\n",
    "\n",
    "        # Run test cases in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor(\n",
    "            max_workers=self.max_concurrent_tasks\n",
    "        ) as executor:\n",
    "            # Submit all test case execution tasks\n",
    "            future_to_test_case = {\n",
    "                executor.submit(\n",
    "                    self.run_test_case,\n",
    "                    test_case,\n",
    "                    run_prompt_function,\n",
    "                    extra_criteria,\n",
    "                ): test_case\n",
    "                for test_case in dataset\n",
    "            }\n",
    "\n",
    "            # Collect results as they complete\n",
    "            for future in concurrent.futures.as_completed(future_to_test_case):\n",
    "                result = future.result()\n",
    "                completed += 1\n",
    "                current_percentage = int((completed / total) * 100)\n",
    "                milestone_percentage = (current_percentage // 20) * 20\n",
    "\n",
    "                # Print progress at 20% milestones\n",
    "                if milestone_percentage > last_reported_percentage:\n",
    "                    print(f\"Graded {completed}/{total} test cases\")\n",
    "                    last_reported_percentage = milestone_percentage\n",
    "                results.append(result)\n",
    "\n",
    "        # Compute and print overall average score\n",
    "        average_score = mean([result[\"score\"] for result in results])\n",
    "        print(f\"Average score: {average_score}\")\n",
    "\n",
    "        # Save results to JSON file\n",
    "        with open(json_output_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        # Generate and save HTML report\n",
    "        html = generate_prompt_evaluation_report(results)\n",
    "        with open(html_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html)\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of PromptEvaluator for test generation and evaluation\n",
    "# Note: Increase `max_concurrent_tasks` for more parallelism, but be mindful of API rate limits!\n",
    "evaluator = PromptEvaluator(max_concurrent_tasks=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a test dataset using the evaluator\n",
    "# The dataset will be used to evaluate your prompt across diverse test cases\n",
    "\n",
    "dataset = evaluator.generate_dataset(\n",
    "    # Describe the purpose or goal of the prompt you're trying to test\n",
    "    task_description=\"\",\n",
    "    # Describe the different inputs that your prompt requires\n",
    "    prompt_inputs_spec={},\n",
    "    # Where to write the generated dataset\n",
    "    output_file=\"dataset.json\",\n",
    "    # Number of test cases to generate (recommend keeping this low if you're getting rate limit errors)\n",
    "    num_cases=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt you want to test\n",
    "# This function will be called once for each test case in the dataset\n",
    "# It should accept prompt_inputs and return the raw model output\n",
    "\n",
    "def run_prompt(prompt_inputs):\n",
    "    \"\"\"\n",
    "    Execute the prompt being evaluated.\n",
    "    \n",
    "    Args:\n",
    "        prompt_inputs: Dictionary of input values for this test case\n",
    "    \n",
    "    Returns:\n",
    "        Raw text output from the model\n",
    "    \"\"\"\n",
    "    # Build the prompt string using the test case inputs\n",
    "    # Customize this prompt based on your task requirements\n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize message history and add the user's prompt\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    \n",
    "    # Send to model and return the response text\n",
    "    return chat(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full evaluation suite on the generated dataset\n",
    "# This executes the run_prompt function for each test case, grades the results, and generates reports\n",
    "\n",
    "results = evaluator.run_evaluation(\n",
    "    run_prompt_function=run_prompt, \n",
    "    dataset_file=\"dataset.json\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
