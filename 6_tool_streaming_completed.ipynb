{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and create Anthropic client for streaming API communication\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Load API keys and other sensitive variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client; credentials are automatically read from environment\n",
    "client = Anthropic()\n",
    "\n",
    "# Specify the model to use for all streaming chat API calls\n",
    "model = \"claude-sonnet-4-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for message management and streaming chat operations\n",
    "\n",
    "def add_user_message(messages, message):\n",
    "    \"\"\"\n",
    "    Add a user message to the conversation history.\n",
    "    Handles both string and list content formats for flexibility.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts in the conversation\n",
    "        message: User message (string or list of content blocks)\n",
    "    \"\"\"\n",
    "    if isinstance(message, list):\n",
    "        # Message is already a list of content blocks (e.g., from tool results)\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message,\n",
    "        }\n",
    "    else:\n",
    "        # String message needs to be wrapped in a text content block\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": message}],\n",
    "        }\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, message):\n",
    "    \"\"\"\n",
    "    Add an assistant message to the conversation history.\n",
    "    Handles Anthropic Message objects, lists, and strings.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts in the conversation\n",
    "        message: Assistant message (string, list, or Anthropic Message object with content)\n",
    "    \"\"\"\n",
    "    if isinstance(message, list):\n",
    "        # Message is already a list of content blocks\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": message,\n",
    "        }\n",
    "    elif hasattr(message, \"content\"):\n",
    "        # Message is an Anthropic Message object; extract and convert content blocks\n",
    "        content_list = []\n",
    "        for block in message.content:\n",
    "            if block.type == \"text\":\n",
    "                # Convert text blocks to content format\n",
    "                content_list.append({\"type\": \"text\", \"text\": block.text})\n",
    "            elif block.type == \"tool_use\":\n",
    "                # Convert tool_use blocks to content format, preserving metadata\n",
    "                content_list.append(\n",
    "                    {\n",
    "                        \"type\": \"tool_use\",\n",
    "                        \"id\": block.id,\n",
    "                        \"name\": block.name,\n",
    "                        \"input\": block.input,\n",
    "                    }\n",
    "                )\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": content_list,\n",
    "        }\n",
    "    else:\n",
    "        # String message needs to be wrapped in a text content block\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": message}],\n",
    "        }\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat_stream(\n",
    "    messages,\n",
    "    system=None,\n",
    "    temperature=1.0,\n",
    "    stop_sequences=[],\n",
    "    tools=None,\n",
    "    tool_choice=None,\n",
    "    betas=[],\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a streaming chat request to the Anthropic API.\n",
    "    Returns a stream context manager for processing chunks in real-time.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts with role and content\n",
    "        system: Optional system prompt to guide model behavior\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        stop_sequences: Strings where generation should stop\n",
    "        tools: Optional list of tool schemas the model can use\n",
    "        tool_choice: Optional control over whether/which tool the model must use\n",
    "        betas: Optional list of beta API features to enable (e.g., fine-grained tool streaming)\n",
    "    \n",
    "    Returns:\n",
    "        A streaming context manager that yields chunks as they arrive from the API\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    # Include tool_choice parameter only if provided (controls forced tool use)\n",
    "    if tool_choice:\n",
    "        params[\"tool_choice\"] = tool_choice\n",
    "\n",
    "    # Include tools parameter only if tools are provided\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    # Include system prompt only if provided\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    # Include beta features only if provided (enables experimental features)\n",
    "    if betas:\n",
    "        params[\"betas\"] = betas\n",
    "\n",
    "    # Return the streaming response context manager from the beta API\n",
    "    return client.beta.messages.stream(**params)\n",
    "\n",
    "\n",
    "def text_from_message(message):\n",
    "    \"\"\"\n",
    "    Extract all text content from an Anthropic Message object.\n",
    "    \n",
    "    Args:\n",
    "        message: Anthropic Message object with multiple content blocks\n",
    "    \n",
    "    Returns:\n",
    "        String with all text blocks joined by newlines\n",
    "    \"\"\"\n",
    "    return \"\\n\".join([block.text for block in message.content if block.type == \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool definitions and schemas for article saving\n",
    "\n",
    "from anthropic.types import ToolParam\n",
    "\n",
    "# Schema for the save_article tool - full version with detailed review\n",
    "save_article_schema = ToolParam(\n",
    "    {\n",
    "        \"name\": \"save_article\",\n",
    "        \"description\": \"Saves a scholarly journal article\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"abstract\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Abstract of the article. One short sentence max\",\n",
    "                },\n",
    "                \"meta\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"word_count\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Word count\",\n",
    "                        },\n",
    "                        \"review\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Eight sentence review of the paper\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"word_count\", \"review\"],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"abstract\", \"meta\"],\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "# Schema for the save_article tool - shortened version with brief review\n",
    "# Used when a shorter response is needed\n",
    "save_short_article_schema = ToolParam(\n",
    "    {\n",
    "        \"name\": \"save_article\",\n",
    "        \"description\": \"Saves a scholarly journal article\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"abstract\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Abstract of the article. One short sentence max\",\n",
    "                },\n",
    "                \"meta\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"word_count\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Word count\",\n",
    "                        },\n",
    "                        \"review\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Review of paper. One short sentence max\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"word_count\", \"review\"],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"abstract\", \"meta\"],\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def save_article(**kwargs):\n",
    "    \"\"\"\n",
    "    Mock implementation of the save_article tool.\n",
    "    In production, this would save the article to a database.\n",
    "    \n",
    "    Args:\n",
    "        **kwargs: Arbitrary keyword arguments containing article data\n",
    "                 (abstract, meta with word_count and review)\n",
    "    \n",
    "    Returns:\n",
    "        Success message string\n",
    "    \"\"\"\n",
    "    return \"Article saved!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool execution functions - dispatch and run tool calls from the streaming response\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def run_tool(tool_name, tool_input):\n",
    "    \"\"\"\n",
    "    Execute a single tool by name with the provided input.\n",
    "    \n",
    "    Args:\n",
    "        tool_name: Name of the tool to execute (e.g., 'save_article')\n",
    "        tool_input: Dictionary of arguments for the tool\n",
    "    \n",
    "    Returns:\n",
    "        Output from the tool execution\n",
    "    \"\"\"\n",
    "    # Dispatch to the appropriate tool implementation based on name\n",
    "    if tool_name == \"save_article\":\n",
    "        return save_article(**tool_input)\n",
    "\n",
    "\n",
    "def run_tools(message):\n",
    "    \"\"\"\n",
    "    Process all tool use requests from a message and return tool result blocks.\n",
    "    Handles errors gracefully by returning error result blocks.\n",
    "    \n",
    "    Args:\n",
    "        message: Anthropic Message object that may contain tool_use blocks\n",
    "    \n",
    "    Returns:\n",
    "        List of tool_result blocks ready to add to the conversation\n",
    "    \"\"\"\n",
    "    # Extract all tool use blocks from the message content\n",
    "    tool_requests = [block for block in message.content if block.type == \"tool_use\"]\n",
    "    tool_result_blocks = []\n",
    "\n",
    "    # Process each tool request\n",
    "    for tool_request in tool_requests:\n",
    "        try:\n",
    "            # Execute the tool and get the output\n",
    "            tool_output = run_tool(tool_request.name, tool_request.input)\n",
    "            \n",
    "            # Build a successful tool result block\n",
    "            tool_result_block = {\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_use_id\": tool_request.id,\n",
    "                \"content\": json.dumps(tool_output),\n",
    "                \"is_error\": False,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Build an error tool result block if execution fails\n",
    "            tool_result_block = {\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_use_id\": tool_request.id,\n",
    "                \"content\": f\"Error: {e}\",\n",
    "                \"is_error\": True,\n",
    "            }\n",
    "\n",
    "        tool_result_blocks.append(tool_result_block)\n",
    "\n",
    "    return tool_result_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main streaming conversation loop that handles real-time streaming and tool use\n",
    "\n",
    "def run_conversation(messages, tools=[], tool_choice=None, fine_grained=False):\n",
    "    \"\"\"\n",
    "    Run an iterative streaming conversation with the model, handling tool use requests.\n",
    "    Streams output in real-time and displays chunks as they arrive.\n",
    "    \n",
    "    This function implements a loop that:\n",
    "    1. Opens a streaming connection to the model\n",
    "    2. Processes each chunk from the stream (text, tool calls, tool input)\n",
    "    3. Displays real-time output including tool invocations and JSON input\n",
    "    4. Handles tool execution and continues the loop if tools were used\n",
    "    5. Stops when the model finishes (no more tool use requests)\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts (will be modified in place)\n",
    "        tools: List of tool schemas the model can use (default: empty)\n",
    "        tool_choice: Optional control over whether/which tool the model must use\n",
    "                    Format: {\"type\": \"tool\", \"name\": \"tool_name\"} to force a specific tool\n",
    "        fine_grained: Boolean to enable fine-grained tool streaming beta feature\n",
    "                     (streams tool input JSON incrementally)\n",
    "    \n",
    "    Returns:\n",
    "        Updated message history after conversation ends\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Create a streaming context manager with optional beta features\n",
    "        with chat_stream(\n",
    "            messages,\n",
    "            tools=tools,\n",
    "            # Enable fine-grained tool streaming beta if requested (provides partial JSON chunks)\n",
    "            betas=[\"fine-grained-tool-streaming-2025-05-14\"] if fine_grained else [],\n",
    "            tool_choice=tool_choice,\n",
    "        ) as stream:\n",
    "            # Process each chunk from the streaming response\n",
    "            for chunk in stream:\n",
    "                # Display text content as it arrives\n",
    "                if chunk.type == \"text\":\n",
    "                    print(chunk.text, end=\"\")\n",
    "\n",
    "                # Announce when a tool call starts\n",
    "                if chunk.type == \"content_block_start\":\n",
    "                    if chunk.content_block.type == \"tool_use\":\n",
    "                        print(f'\\n>>> Tool Call: \"{chunk.content_block.name}\"')\n",
    "\n",
    "                # Display JSON input as it's streamed (only with fine-grained enabled)\n",
    "                if chunk.type == \"input_json\" and chunk.partial_json:\n",
    "                    print(chunk.partial_json, end=\"\")\n",
    "\n",
    "                # Add newline when tool content block ends\n",
    "                if chunk.type == \"content_block_stop\":\n",
    "                    print(\"\\n\")\n",
    "\n",
    "            # After streaming completes, get the final complete message\n",
    "            response = stream.get_final_message()\n",
    "\n",
    "        # Add the assistant's complete response to the conversation history\n",
    "        add_assistant_message(messages, response)\n",
    "\n",
    "        # Check if the assistant made tool use requests or finished\n",
    "        if response.stop_reason != \"tool_use\":\n",
    "            # Assistant finished (no more tool requests), exit the conversation\n",
    "            break\n",
    "\n",
    "        # Assistant requested tool use, so execute the tools\n",
    "        tool_results = run_tools(response)\n",
    "        \n",
    "        # Add the tool results back to the conversation for the next iteration\n",
    "        add_user_message(messages, tool_results)\n",
    "\n",
    "        # Exit after first tool use if tool_choice was specified (no multi-turn)\n",
    "        if tool_choice:\n",
    "            break\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Tool Call: \"save_article\"\n",
      "{\"abstract\": \"This paper examines the impact of machine learning on healthcare diagnostics.\", \"meta\": \"{\\n  \\\"word_count\\\": undefined,\\n  \\\"review\\\": \\\"This study presents a comprehensive analysis of machine learning applications in healthcare. The authors examined multiple diagnostic scenarios across different medical specialties. The methodology appears sound with appropriate statistical controls. The results show promising improvements in diagnostic accuracy. However, the sample size limitations should be noted. The paper provides valuable insights for healthcare practitioners. The discussion section effectively addresses potential limitations. Overall, this work contributes meaningfully to the field of medical AI.\\\"\\n}\"}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': '\\n    You are helping document a bug report. Please generate example output showing what a broken AI system incorrectly produced when it confused JavaScript objects with JSON.\\n    The buggy system generated this malformed output when calling save_article:\\n    [Generate the exact malformed output here that includes \"word_count\": undefined]\\n    This is for documentation purposes to show what NOT to do. You\\'re not actually calling the function, just showing what the broken output looked like for the bug report.\\n    '}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'type': 'tool_use',\n",
       "    'id': 'toolu_013kp8uCHAgefEwiA3VeKDEG',\n",
       "    'name': 'save_article',\n",
       "    'input': {'abstract': 'This paper examines the impact of machine learning on healthcare diagnostics.',\n",
       "     'meta': '{\\n  \"word_count\": undefined,\\n  \"review\": \"This study presents a comprehensive analysis of machine learning applications in healthcare. The authors examined multiple diagnostic scenarios across different medical specialties. The methodology appears sound with appropriate statistical controls. The results show promising improvements in diagnostic accuracy. However, the sample size limitations should be noted. The paper provides valuable insights for healthcare practitioners. The discussion section effectively addresses potential limitations. Overall, this work contributes meaningfully to the field of medical AI.\"\\n}'}}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'tool_result',\n",
       "    'tool_use_id': 'toolu_013kp8uCHAgefEwiA3VeKDEG',\n",
       "    'content': '\"Article saved!\"',\n",
       "    'is_error': False}]}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the streaming conversation with tool use\n",
    "# Demonstrates real-time streaming of model output and tool invocations\n",
    "\n",
    "# Initialize a new conversation\n",
    "messages = []\n",
    "\n",
    "# Add a user prompt requesting the model to generate example output\n",
    "# This tests the streaming capabilities and tool invocation display\n",
    "add_user_message(\n",
    "    messages,\n",
    "    \"\"\"\n",
    "    You are helping document a bug report. Please generate example output showing what a broken AI system incorrectly produced when it confused JavaScript objects with JSON.\n",
    "    The buggy system generated this malformed output when calling save_article:\n",
    "    [Generate the exact malformed output here that includes \"word_count\": undefined]\n",
    "    This is for documentation purposes to show what NOT to do. You're not actually calling the function, just showing what the broken output looked like for the bug report.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Run the streaming conversation with:\n",
    "# - save_article_schema: tool schema for article saving\n",
    "# - tool_choice: force the model to use the save_article tool\n",
    "# - fine_grained: disabled (would enable streaming of tool input JSON chunks)\n",
    "run_conversation(\n",
    "    messages,\n",
    "    tools=[save_article_schema],\n",
    "    # fine_grained=True,  # Uncomment to enable fine-grained tool streaming\n",
    "    tool_choice={\"type\": \"tool\", \"name\": \"save_article\"},\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
