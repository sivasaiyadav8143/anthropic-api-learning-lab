{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# macOS (zsh)\n",
    "# python -m pip install --upgrade pip\n",
    "# python -m pip install python-dotenv anthropic\n",
    "\n",
    "# Load env variables and create client\n",
    "# Use python-dotenv to load environment variables (for example API keys) from a .env file\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Read environment variables from a local .env into os.environ\n",
    "load_dotenv()\n",
    "\n",
    "# Instantiate the Anthropic client; the client will read credentials from the environment\n",
    "client = Anthropic()\n",
    "\n",
    "# Select which model to use for chat calls\n",
    "model = \"claude-haiku-4-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for building messages and making chat calls to the model\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    # Create and append a user message dict to the messages list\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    # Create and append an assistant message dict to the messages list\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    \"\"\"\n",
    "    Send a chat request to the Anthropic client and return the textual response.\n",
    "\n",
    "    Args:\n",
    "        messages: a list of message dicts constructed by the helper functions\n",
    "        system: optional system prompt to pass to the model\n",
    "        temperature: sampling temperature\n",
    "        stop_sequences: list of sequences where the model should stop\n",
    "\n",
    "    Returns:\n",
    "        The model-generated text (string).\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        # Only include the system parameter when provided\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    # Call the Anthropic client and extract the text portion of the response\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e788701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a new dataset by prompting the model\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    # The prompt instructs the model to return a JSON array of task objects.\n",
    "    # Each object should contain a task description, a required output format (json/python/regex),\n",
    "    # and solution criteria to use when grading.\n",
    "    prompt = \"\"\"\n",
    "Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"json\" or \"python\" or \"regex\",\n",
    "        \"solution_criteria\": \"Key criteria for evaluating the solution\"\n",
    "    },\n",
    "    ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\"\n",
    "\n",
    "    # Build the messages list and ask the assistant to return a JSON block\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    # We add an assistant marker to encourage the model to return a code block\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "\n",
    "    # Call the chat helper and stop at the closing code fence\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "\n",
    "    # Parse the returned JSON string into Python objects and return\n",
    "    return json.loads(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ed743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset and write it to 'dataset.json' for later use\n",
    "# This runs the generator above and persists the results so tests can be re-run deterministically.\n",
    "\n",
    "dataset = generate_dataset()\n",
    "with open(\"dataset.json\", \"w\") as f:\n",
    "    # Write a human-readable JSON file with indentation\n",
    "    json.dump(dataset, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b89174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grade a test case's output using the model as an evaluator\n",
    "# The evaluator is asked to return a concise JSON object describing strengths, weaknesses, reasoning and a score.\n",
    "\n",
    "def grade_by_model(test_case, output):\n",
    "    # Build an evaluation prompt that includes the task, the candidate solution, and the criteria\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "Original Task:\n",
    "<task>\n",
    "{test_case[\"task\"]}\n",
    "</task>\n",
    "\n",
    "Solution to Evaluate:\n",
    "<solution>\n",
    "{output}\n",
    "</solution>\n",
    "\n",
    "Criteria you should use to evaluate the solution:\n",
    "<criteria>\n",
    "{test_case[\"solution_criteria\"]}\n",
    "</criteria>\n",
    "\n",
    "Output Format\n",
    "Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "- \"strengths\": An array of 1-3 key strengths\n",
    "- \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "- \"reasoning\": A concise explanation of your overall assessment\n",
    "- \"score\": A number between 1-10\n",
    "\n",
    "Respond with JSON. Keep your response concise and direct.\n",
    "Example response shape:\n",
    "{{\n",
    "    \"strengths\": string[],\n",
    "    \"weaknesses\": string[],\n",
    "    \"reasoning\": string,\n",
    "    \"score\": number\n",
    "}}\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the evaluation prompt to the model and parse the returned JSON\n",
    "    messages = []\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83809a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passes a test case into Claude (or other model) and returns the raw model output\n",
    "def run_prompt(test_case):\n",
    "    # Build a concise prompt instructing the model to only emit the requested format\n",
    "    prompt = f\"\"\"\n",
    "Please solve the following task:\n",
    "\n",
    "{test_case[\"task\"]}\n",
    "\n",
    "* Respond only with Python, JSON, or a plain Regex\n",
    "* Do not add any comments or commentary or explanation\n",
    "\"\"\"\n",
    "\n",
    "    # Use the helper functions to prepare messages and request a code-style response\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```code\")\n",
    "\n",
    "    # Ask the model and stop at the closing code fence so we capture only the code/text\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to validate the output structure produced by the model\n",
    "# Each validator returns 10 for valid syntax or 0 for invalid syntax, so they can be averaged with model scores.\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def validate_json(text):\n",
    "    # Try to parse the text as JSON; strip whitespace first\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    # Try to parse the text into a Python AST; it verifies basic syntax validity\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    # Try to compile the regex to ensure it is syntactically valid\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    # Choose the correct syntax validator based on the expected format of the test case\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a single test case and grade the output\n",
    "# Steps:\n",
    "# 1. Call the model to get a candidate solution\n",
    "# 2. Ask the model to evaluate the candidate solution (grade_by_model)\n",
    "# 3. Validate the syntax of the candidate solution (grade_syntax)\n",
    "# 4. Combine the model's evaluation score and the syntax score into a final score\n",
    "\n",
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    # 1) Get the model's output for the test case\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    # 2) Use the model as an evaluator to grade the solution semantically\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade.get(\"reasoning\", \"\")\n",
    "\n",
    "    # 3) Check syntax validity based on expected format (json/python/regex)\n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "\n",
    "    # 4) Average the model's score with the syntax score to produce a final numeric score\n",
    "    score = (model_score + syntax_score) / 2\n",
    "\n",
    "    # Return a structured result containing the output, metadata and score\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa99d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "\n",
    "# Run the full evaluation suite across the provided dataset\n",
    "# Returns a list of result dicts and prints an average score\n",
    "def run_eval(dataset):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in dataset:\n",
    "        # Run and grade each test case\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "\n",
    "    # Compute the average score across all test cases (uses the 'score' key)\n",
    "    average_score = mean([result[\"score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fae983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 8.166666666666666\n"
     ]
    }
   ],
   "source": [
    "# Load the previously generated dataset from disk and run the evaluation suite\n",
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Execute the evaluation and collect results\n",
    "results = run_eval(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc6111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"\\n{\\n    \\\"lambda_function\\\": {\\n        \\\"FunctionName\\\": \\\"database-connection-handler\\\",\\n        \\\"Runtime\\\": \\\"python3.9\\\",\\n        \\\"Environment\\\": {\\n            \\\"Variables\\\": {\\n                \\\"DB_HOST\\\": \\\"your-database-hostname.rds.amazonaws.com\\\",\\n                \\\"DB_PORT\\\": \\\"5432\\\",\\n                \\\"DB_NAME\\\": \\\"myappdatabase\\\",\\n                \\\"DB_USERNAME\\\": \\\"dbadminuser\\\", \\n                \\\"DB_PASSWORD\\\": \\\"{{resolve:secretsmanager:DatabaseCredentials:SecretString:password}}\\\",\\n                \\\"DB_SSL_MODE\\\": \\\"require\\\"\\n            }\\n        },\\n        \\\"Timeout\\\": 30,\\n        \\\"MemorySize\\\": 256\\n    }\\n}\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON configuration for an AWS Lambda function that sets environment variables for database connection\",\n",
      "      \"format\": \"json\"\n",
      "    },\n",
      "    \"score\": 8.5,\n",
      "    \"reasoning\": \"The solution demonstrates good foundational practices for Lambda environment configuration with database connection variables, but lacks advanced security and infrastructure-as-code best practices. The use of Secrets Manager is a strong point, but the implementation could be more dynamic and secure.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\ndef extract_aws_region(arn):\\n    import re\\n    match = re.search(r'arn:aws:.*:([^:]+):', arn)\\n    return match.group(1) if match else None\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function to extract the AWS region from an ARN (Amazon Resource Name) string\",\n",
      "      \"format\": \"python\"\n",
      "    },\n",
      "    \"score\": 8.0,\n",
      "    \"reasoning\": \"The solution provides a basic, functional approach to extracting AWS region from an ARN using regex. While concise, it lacks robust error handling and comprehensive ARN validation. The regex pattern is relatively simple and might miss some complex ARN formats.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n^i-[a-zA-Z0-9]+$\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Design a regular expression to validate a valid AWS EC2 instance ID (format: i-[alphanumeric characters])\",\n",
      "      \"format\": \"regex\"\n",
      "    },\n",
      "    \"score\": 8.0,\n",
      "    \"reasoning\": \"The regex captures the basic structure of an EC2 instance ID but lacks precision for AWS's specific requirements. A more robust solution would include length constraints and potentially additional validation.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Pretty-print the results as JSON so they're easy to read in notebook output\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
